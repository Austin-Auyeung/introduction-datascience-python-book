{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Recommender Systems.\n",
    "\n",
    "In this notebook we will see how to create a recommender system. To\n",
    "this end we will explore different properties of recommender systems, develope and evaluate a collaborative recommender system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4c/yxd8svq13sj393bkl75mt6km0000gn/T/ipykernel_2329/3466072315.py:4: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline \n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='times')\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plt.rc('font', size=12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "We will work with the well known MovieLens dataset (http://grouplens.org/datasets/movielens/). This dataset was initially constructed to support participants in the Netflix Prize. Today, we can find several versions of this dataset with different amout of data, from 100k samples version to 20m sample version.\n",
    "Although performance on bigger dataset is expected to be better, we will work with the smallest dataset: MovieLens *100K Dataset (ml-100k-zip)*. Working with this lite version has the benefit of less computational costs, while we will also get the skills required on user-based recommender systems.\n",
    "\n",
    "With a unix machine the dataset can be downloaded with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "unzip:  cannot find or open ml-100k.zip, ml-100k.zip.zip or ml-100k.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip \n",
    "!unzip ml-100k.zip -d \"files/ch09/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working with a windows machine, please go to the website and download the 100k version and extract it to the subdirectory named \"files/ch8/ml-100k/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have downloaded and unzipped the file into a directory, you can create a DataFrame with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database has 100000 observed ratings\n",
      "The database has  943  users\n",
      "The database has  1682  movies\n",
      "   user_id         title  movie_id  rating\n",
      "0      196  Kolya (1996)       242       3\n",
      "1      305  Kolya (1996)       242       5\n",
      "2        6  Kolya (1996)       242       4\n",
      "3      234  Kolya (1996)       242       4\n",
      "4       63  Kolya (1996)       242       3\n"
     ]
    }
   ],
   "source": [
    "#NETFLIX REAL 50.000.000 usuaris and 100.000 items\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "from math import isnan\n",
    "from tqdm import tqdm # conda install -y tqdm\n",
    "\n",
    "\n",
    "# Load Data set\n",
    "u_cols = [\n",
    "    'user_id', 'age', 'sex', \n",
    "    'occupation', 'zip_code']\n",
    "\n",
    "users = pd.read_csv('files/ch09/ml-100k/u.user', \n",
    "                    sep='|', \n",
    "                    names=u_cols)\n",
    "\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('files/ch09/ml-100k/u.data', sep='\\t', names=r_cols)\n",
    "\n",
    "# We will only load the first three columns of the filewith usecols\n",
    "m_cols = ['movie_id', 'title', 'release_date']\n",
    "movies = pd.read_csv('files/ch09/ml-100k/u.item', sep='|', names=m_cols, usecols=range(3), encoding='latin-1')\n",
    "\n",
    "# Construcció del DataFrame\n",
    "data = pd.merge(pd.merge(ratings, users), movies)\n",
    "data = data[['user_id','title', 'movie_id','rating']]\n",
    "print(\"The database has \"+ str(data.shape[0]) +\" observed ratings\")\n",
    "print(\"The database has \", data.user_id.nunique(),\" users\")\n",
    "print(\"The database has \", data.movie_id.nunique(), \" movies\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you explore the dataset in detail, you will see that it  consists of:\n",
    "<ul>\n",
    "<li>1,000,209 ratings from 6040 users of 3706 movies. Ratings are from 1 to 5.</li>\n",
    "<li>Each user has rated at least 20 movies.</li>\n",
    "<li>Simple demographic info for the users (age, gender, occupation, zip)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Collaborative Filtering (CF) for Movilens dataset\n",
    "The idea behind any recommender system is to suggest or recommend items that are likely to be of interest to the user. If we think on the movielens dataset and a movie recommender system, the goal will consist on recommend those useen movies by a given \"user\" that are more likely to be intersting to him or her. So, the problem can be taken by predicting the ratings for the unseen movies of the given \"user\" and recommend those with the highest predicted rating.\n",
    "\n",
    "<br>The following table ilustrates the problem. This toy dataset consists on 4 users and 4 items.  Users 1, 2 and 3 have seen all the movies while user 3 has only seen Superman and Star Wars. So, the problem consist on prediction which movie, Matrix or Spiderman, will be more likely for the user 4.\n",
    "<table style=\"width:60%\">\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>Superman</td> \n",
    "    <td>Star Wars 1</td>\n",
    "    <td>Matrix</td>\n",
    "    <td>Spiderman</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>user1</td>\n",
    "    <td>3</td> \n",
    "    <td>3.5</td>\n",
    "    <td>4.5</td>\n",
    "    <td>3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>user2</td>\n",
    "    <td>3.5</td> \n",
    "    <td>4</td>\n",
    "    <td>5</td>\n",
    "    <td>5</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>user3</td>\n",
    "    <td>3</td> \n",
    "    <td>4</td>\n",
    "    <td>4.5</td>\n",
    "    <td>3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>user4</td>\n",
    "    <td>3.5</td> \n",
    "    <td>5</td>\n",
    "    <td><font color=\"red\"><b>¿?</b></font></td>\n",
    "    <td><font color=\"red\"><b>¿?</b></font></td>\n",
    "  </tr>\n",
    "</table>\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "In order to create a collaborative recommender system we will have to define: 1) a prediction function, 2) a user-similarity function and 3) an evaluation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function:\n",
    "The prediction function behind the CF will be based on the movie ratings from similar users.\n",
    "So, in order to recommend a movie, $p$, from a set of movies, $P$, to a given user, $a$, we first need to see the set of users, $B$, who have already seen $p$. Then, we need to see the taste similarity between these users in $B$ and user $a$. The most simple prediction function for a user $a$ and movie $p$ can be defined as follows:\n",
    "\n",
    "$$pred(a,p) = \\frac{\\sum_{b \\in B}{sim(a,b)*(r_{b,p})}}{\\sum_{b \\in B}{sim(a,b)}}$$\n",
    "\n",
    "where $sim(a,b)$ is the similarity between user $a$ and user $b$,  $B$ is the set of users in the dataset that have already seen $p$ and $r_{b,p}$ is the rating of $p$ by $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Users similarity</h3>\n",
    "The computation of the similarity between items is one of the most critical steps in the CF algorithms. The basic idea behind the similarity computation between two users $a$ and $b$, is that we can first isolate the set $P$ of items rated by both users, and then apply a similarity computation technique to determine the similarity.\n",
    "\n",
    "The set of *common_movies* can be obtained with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of common movies 96 \n",
      "\n",
      "                                    title  rating\n",
      "14                           Kolya (1996)       5\n",
      "940                Raising Arizona (1987)       4\n",
      "1306  Truth About Cats & Dogs, The (1996)       5\n",
      "2677                          Babe (1995)       1\n",
      "3073   Four Weddings and a Funeral (1994)       3\n",
      "                                    title  rating\n",
      "2                            Kolya (1996)       4\n",
      "885                Raising Arizona (1987)       5\n",
      "1255  Truth About Cats & Dogs, The (1996)       2\n",
      "2636                          Babe (1995)       4\n",
      "3022   Four Weddings and a Funeral (1994)       3\n"
     ]
    }
   ],
   "source": [
    "# dataframe with the data from user 1\n",
    "data_user_1 = data[data.user_id==1]\n",
    "\n",
    "# dataframe with the data from user 2\n",
    "data_user_2 = data[data.user_id==6]\n",
    "\n",
    "# We first compute the set of common movies\n",
    "common_movies = set(data_user_1.movie_id).intersection(data_user_2.movie_id)\n",
    "print(\"\\nNumber of common movies\",\n",
    "      len(common_movies),'\\n')\n",
    "\n",
    "# Sub-dataframe with only the common movies \n",
    "mask = (data_user_1.movie_id.isin(common_movies))\n",
    "data_user_1 = data_user_1[mask]\n",
    "print(data_user_1[['title','rating']].head())\n",
    "\n",
    "mask = (data_user_2.movie_id.isin(common_movies))\n",
    "data_user_2 = data_user_2[mask]\n",
    "print(data_user_2[['title','rating']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the set of ratings for all movies common to the two users has been obtained, we can compute the user similarity. These are some of the most common similarity functions used in CF methods: \n",
    "\n",
    "   <ul>\n",
    "    <li>Euclidean distance</li>\n",
    "    $$sim(a,b) = \\frac{1}{1+\\sqrt{\\sum_{p \\in P}{(r_{a,p} - r_{b,p})^2}}}$$\n",
    "    <br>\n",
    "    <li>Pearson Correlation</li>\n",
    "    $$sim(a,b) = \\frac{\\sum_{p\\in P} (r_{a,p}-\\bar{r_a})(r_{b,p}-\\bar{r_b})}{\\sqrt{\\sum_{p \\in P}(r_{a,p}-\\bar{r_a})²}\\sqrt{\\sum_{p \\in P}(r_{b,p}-\\bar{r_b})²}}$$\n",
    "    <br>\n",
    "    <li>Cosine distance</li>\n",
    "    $$ sim(a,b) = \\frac{\\vec{a}· \\vec{b}}{|\\vec{a}| * |\\vec{b}|}$$\n",
    "    <br>\n",
    "    </ul>\n",
    "  \n",
    "<br>\n",
    "Where: \n",
    "\n",
    "* $sim(a,b)$ is the similarity between user \"a\" and user \"b\"\n",
    "* $P$ is the set of common rated movies by user \"a\" and \"b\"\n",
    "* $r_{a,p}$ is the rating of movie \"p\" by user \"a\"\n",
    "* $\\bar{r_a}$ is the mean rating given by user \"a\"\n",
    "\n",
    "<br>\n",
    "\n",
    "<h4>Some issues to take into accout</h4>\n",
    "<ul>\n",
    "<li>Pearson Correlation used to work better than euclidean distance since it is based more on the ranking than on the values.</li>\n",
    "<li>Cosine distance is usually used when our data is binary/unary, i.e. like vs. not like  or buy vs. not buy.</li>\n",
    "<li>What happens if two users have very few items in common?</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a function to compute the user's similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Returns a user-based similarity score for two users\n",
    "# Person correlation is set as the default\n",
    "def user_sim(df, user_1, user_2, \n",
    "                    min_common_items = 3, \n",
    "                    method = 'pearson'):\n",
    "    # GET MOVIES OF USER1\n",
    "    mov_usr1 = df[df['user_id'] == user_1 ]\n",
    "    # GET MOVIES OF USER2\n",
    "    mov_usr2 = df[df['user_id'] == user_2 ]\n",
    "    \n",
    "    # FIND SHARED FILMS\n",
    "    df_shared = pd.merge(mov_usr1, mov_usr2, on='movie_id')  \n",
    "    \n",
    "    # If there is no enough common items to comput similarity\n",
    "    if(df_shared.shape[0]<=min_common_items):\n",
    "        return 0\n",
    "    \n",
    "    if method =='pearson':\n",
    "        res = pearsonr(df_shared['rating_x'],df_shared['rating_y'])[0]\n",
    "        if(np.isnan(res)):\n",
    "            return 0\n",
    "        return res\n",
    "    \n",
    "    elif method =='euclidean':\n",
    "        return 1.0/(1.0+euclidean(df_shared['rating_x'],\n",
    "                                  df_shared['rating_y'])) \n",
    "    else:\n",
    "        print(\"method not defined\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how similars are user 1 with 8 and 1 with 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Similarity 0.1566130288262323\n",
      "Pearson Similarity 0.6920863660773593\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean Similarity\",user_sim(data,1,8,\n",
    "                                             method = 'euclidean') )\n",
    "print(\"Pearson Similarity\",user_sim(data,1,8) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Similarity 0.1757340838011157\n",
      "Pearson Similarity -0.09221388919541465\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean Similarity\",user_sim(data,1,31,\n",
    "                                             method = 'euclidean') )\n",
    "print(\"Pearson Similarity\",user_sim(data,1,31) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to process string with tex because latex could not be found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/texmanager.py:255\u001b[0m, in \u001b[0;36mTexManager._run_checked_subprocess\u001b[0;34m(cls, command, tex, cwd)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     report \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mcheck_output(\n\u001b[1;32m    256\u001b[0m         command, cwd\u001b[38;5;241m=\u001b[39mcwd \u001b[38;5;28;01mif\u001b[39;00m cwd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mtexcache,\n\u001b[1;32m    257\u001b[0m         stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:466\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m empty\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run(\u001b[38;5;241m*\u001b[39mpopenargs, stdout\u001b[38;5;241m=\u001b[39mPIPE, timeout\u001b[38;5;241m=\u001b[39mtimeout, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    467\u001b[0m            \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'latex'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m printer(obj)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(bytes_io, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/backend_bases.py:2342\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m _get_renderer(\n\u001b[1;32m   2337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[1;32m   2338\u001b[0m         functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m   2339\u001b[0m             print_method, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[1;32m   2340\u001b[0m     )\n\u001b[1;32m   2341\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2342\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m draw(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3175\u001b[0m mimage\u001b[38;5;241m.\u001b[39m_draw_list_compositing_images(\n\u001b[1;32m   3176\u001b[0m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppressComposite)\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3179\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/axes/_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3062\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3064\u001b[0m mimage\u001b[38;5;241m.\u001b[39m_draw_list_compositing_images(\n\u001b[1;32m   3065\u001b[0m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39msuppressComposite)\n\u001b[1;32m   3067\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/axis.py:1377\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1374\u001b[0m renderer\u001b[38;5;241m.\u001b[39mopen_group(\u001b[38;5;18m__name__\u001b[39m, gid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gid())\n\u001b[1;32m   1376\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[0;32m-> 1377\u001b[0m tlb1, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks_to_draw:\n\u001b[1;32m   1380\u001b[0m     tick\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/axis.py:1304\u001b[0m, in \u001b[0;36mAxis._get_ticklabel_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1305\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1306\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1307\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/axis.py:1304\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1305\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1306\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1307\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/text.py:959\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    955\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get window extent of text w/o renderer. You likely \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    956\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwant to call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.draw_without_rendering()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m--> 959\u001b[0m     bbox, info, descent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_layout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer)\n\u001b[1;32m    960\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unitless_position()\n\u001b[1;32m    961\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transform()\u001b[38;5;241m.\u001b[39mtransform((x, y))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/text.py:378\u001b[0m, in \u001b[0;36mText._get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    375\u001b[0m ys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Full vertical extent of font, including ascenders and descenders:\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m _, lp_h, lp_d \u001b[38;5;241m=\u001b[39m _get_text_metrics_with_cache(\n\u001b[1;32m    379\u001b[0m     renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fontproperties,\n\u001b[1;32m    380\u001b[0m     ismath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_usetex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi)\n\u001b[1;32m    381\u001b[0m min_dy \u001b[38;5;241m=\u001b[39m (lp_h \u001b[38;5;241m-\u001b[39m lp_d) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linespacing\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/text.py:97\u001b[0m, in \u001b[0;36m_get_text_metrics_with_cache\u001b[0;34m(renderer, text, fontprop, ismath, dpi)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call ``renderer.get_text_width_height_descent``, caching the results.\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Cached based on a copy of fontprop so that later in-place mutations of\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# the passed-in argument do not mess up the cache.\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_text_metrics_with_cache_impl(\n\u001b[1;32m     98\u001b[0m     weakref\u001b[38;5;241m.\u001b[39mref(renderer), text, fontprop\u001b[38;5;241m.\u001b[39mcopy(), ismath, dpi)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/text.py:105\u001b[0m, in \u001b[0;36m_get_text_metrics_with_cache_impl\u001b[0;34m(renderer_ref, text, fontprop, ismath, dpi)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(\u001b[38;5;241m4096\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_text_metrics_with_cache_impl\u001b[39m(\n\u001b[1;32m    103\u001b[0m         renderer_ref, text, fontprop, ismath, dpi):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# dpi is unused, but participates in cache invalidation (via the renderer).\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m renderer_ref()\u001b[38;5;241m.\u001b[39mget_text_width_height_descent(text, fontprop, ismath)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/backends/backend_agg.py:226\u001b[0m, in \u001b[0;36mRendererAgg.get_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    224\u001b[0m _api\u001b[38;5;241m.\u001b[39mcheck_in_list([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m], ismath\u001b[38;5;241m=\u001b[39mismath)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismath \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeX\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_text_width_height_descent(s, prop, ismath)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismath:\n\u001b[1;32m    229\u001b[0m     ox, oy, width, height, descent, font_image \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmathtext_parser\u001b[38;5;241m.\u001b[39mparse(s, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdpi, prop)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/backend_bases.py:645\u001b[0m, in \u001b[0;36mRendererBase.get_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    641\u001b[0m fontsize \u001b[38;5;241m=\u001b[39m prop\u001b[38;5;241m.\u001b[39mget_size_in_points()\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismath \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeX\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# todo: handle properties\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_texmanager()\u001b[38;5;241m.\u001b[39mget_text_width_height_descent(\n\u001b[1;32m    646\u001b[0m         s, fontsize, renderer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    648\u001b[0m dpi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints_to_pixels(\u001b[38;5;241m72\u001b[39m)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismath:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/texmanager.py:368\u001b[0m, in \u001b[0;36mTexManager.get_text_width_height_descent\u001b[0;34m(cls, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tex\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 368\u001b[0m dvifile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dvi(tex, fontsize)\n\u001b[1;32m    369\u001b[0m dpi_fraction \u001b[38;5;241m=\u001b[39m renderer\u001b[38;5;241m.\u001b[39mpoints_to_pixels(\u001b[38;5;241m1.\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dviread\u001b[38;5;241m.\u001b[39mDvi(dvifile, \u001b[38;5;241m72\u001b[39m \u001b[38;5;241m*\u001b[39m dpi_fraction) \u001b[38;5;28;01mas\u001b[39;00m dvi:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/texmanager.py:300\u001b[0m, in \u001b[0;36mTexManager.make_dvi\u001b[0;34m(cls, tex, fontsize)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TemporaryDirectory(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mcwd) \u001b[38;5;28;01mas\u001b[39;00m tmpdir:\n\u001b[1;32m    299\u001b[0m         tmppath \u001b[38;5;241m=\u001b[39m Path(tmpdir)\n\u001b[0;32m--> 300\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_checked_subprocess(\n\u001b[1;32m    301\u001b[0m             [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatex\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-interaction=nonstopmode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--halt-on-error\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    302\u001b[0m              \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--output-directory=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmppath\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    303\u001b[0m              \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtexfile\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], tex, cwd\u001b[38;5;241m=\u001b[39mcwd)\n\u001b[1;32m    304\u001b[0m         (tmppath \u001b[38;5;241m/\u001b[39m Path(dvifile)\u001b[38;5;241m.\u001b[39mname)\u001b[38;5;241m.\u001b[39mreplace(dvifile)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dvifile\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/texmanager.py:259\u001b[0m, in \u001b[0;36mTexManager._run_checked_subprocess\u001b[0;34m(cls, command, tex, cwd)\u001b[0m\n\u001b[1;32m    255\u001b[0m     report \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mcheck_output(\n\u001b[1;32m    256\u001b[0m         command, cwd\u001b[38;5;241m=\u001b[39mcwd \u001b[38;5;28;01mif\u001b[39;00m cwd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mtexcache,\n\u001b[1;32m    257\u001b[0m         stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to process string with tex because \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m could not be \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(command[\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{prog}\u001b[39;00m\u001b[38;5;124m was not able to process the following string:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{tex!r}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m             exc\u001b[38;5;241m=\u001b[39mexc\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackslashreplace\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    273\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to process string with tex because latex could not be found"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_id_1, user_id_2 = 1, 8\n",
    "movies_user1=data[data['user_id'] ==user_id_1 ][['user_id','movie_id','rating']]\n",
    "movies_user2=data[data['user_id'] ==user_id_2 ][['user_id','movie_id','rating']]\n",
    "    \n",
    "# FIND SHARED FILMS\n",
    "rep=pd.merge(movies_user1 ,movies_user2,on='movie_id')\n",
    "x= rep.rating_x + np.random.normal(loc=0.0, scale=0.1,size=len(rep.rating_x))\n",
    "y= rep.rating_y +np.random.normal(loc=0.0, scale=0.1,size=len(rep.rating_y))\n",
    "    \n",
    "a=rep.groupby(['rating_x', 'rating_y']).size()\n",
    "x=[]\n",
    "y=[]\n",
    "s=[]\n",
    "for item,b in a.items():\n",
    "    x.append(item[0])\n",
    "    y.append(item[1])\n",
    "    s.append(b*30.)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.scatter(x,y, s=s)\n",
    "#plt.xlabel('Rating User 1')\n",
    "#plt.ylabel('Rating User '+str(8))\n",
    "#plt.axis([0.5,5.5,0.5,5.5])\n",
    "#plt.savefig(\"corre18.png\",dpi= 300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, with user 31:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4c/yxd8svq13sj393bkl75mt6km0000gn/T/ipykernel_2329/3896844762.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'iteritems'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_id_1, user_id_2 = 1, 31\n",
    "movies_user1=data[data['user_id'] ==user_id_1 ][['user_id','movie_id','rating']]\n",
    "movies_user2=data[data['user_id'] ==user_id_2 ][['user_id','movie_id','rating']]\n",
    "    \n",
    "# FIND SHARED FILMS\n",
    "rep=pd.merge(movies_user1 ,movies_user2,on='movie_id')\n",
    "x= rep.rating_x + np.random.normal(loc=0.0, scale=0.1,size=len(rep.rating_x))\n",
    "y= rep.rating_y +np.random.normal(loc=0.0, scale=0.1,size=len(rep.rating_y))\n",
    "    \n",
    "a=rep.groupby(['rating_x', 'rating_y']).size()\n",
    "x=[]\n",
    "y=[]\n",
    "s=[]\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "for item,b in a.iteritems():\n",
    "    x.append(item[0])\n",
    "    y.append(item[1])\n",
    "    s.append(b*30)\n",
    "plt.scatter(x,y, s=s)\n",
    "plt.xlabel('Rating User 1')\n",
    "plt.ylabel('Rating User '+str(31))\n",
    "plt.axis([0.5,5.5,0.5,5.5])\n",
    "plt.savefig(\"corre131.png\",dpi= 300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation: performance criterion</h3>\n",
    "In order to validate the system, we will divide the dataset into two different sets: one called \\emph{$X\\_train$} containing  $80\\%$ of the data from each user; and another called \\emph{$X\\_test$}, with the remaining $20\\%$ of the data from each user. In the following code we create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training samples =  79619\n",
      "#Test samples =  20381\n",
      "#Users = 943\n",
      "#Movies = 1651\n"
     ]
    }
   ],
   "source": [
    "def assign_to_set(df):\n",
    "    sampled_ids = np.random.choice(\n",
    "        df.index,\n",
    "        size=np.int64(np.ceil(df.index.size * 0.2)),\n",
    "        replace=False)\n",
    "    df.loc[sampled_ids, 'for_testing'] = True\n",
    "    return df\n",
    "\n",
    "def create_train_test(data, key = 'user_id'):\n",
    "    data['for_testing'] = False\n",
    "    grouped = data.groupby(key, group_keys=False).apply(assign_to_set)\n",
    "    # dataframe used to train our model\n",
    "    data_train = data[grouped.for_testing == False]\n",
    "    # dataframe used to evaluate our model\n",
    "    data_test = data[grouped.for_testing == True]\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "X_train, X_test =  create_train_test(data)\n",
    "\n",
    "print(\"#Training samples = \",X_train.shape[0])\n",
    "print(\"#Test samples = \",X_test.shape[0])\n",
    "print('#Users =', X_train.user_id.nunique())\n",
    "print('#Movies =',X_train.movie_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once the data is divided in these sets, we can build a model with the **training set** and evaluate its performance using the **test set**. To validate our methods we can use the standard Root Mean Square Error (RMSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y_pred, y_true):\n",
    "    \"\"\" Compute Root Mean Squared Error. \"\"\"\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or instead, we can use ranking measures such as Precision, Reacall or Average Precision (AP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(recommended_items, relevant_items):\n",
    "    is_relevant = np.in1d(recommended_items, \n",
    "                          relevant_items, \n",
    "                          assume_unique=True)\n",
    "    \n",
    "    precision = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def recall(recommended_items, relevant_items):  \n",
    "    is_relevant = np.in1d(recommended_items, \n",
    "                          relevant_items, \n",
    "                          assume_unique=True)\n",
    "    \n",
    "    recall = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def AP(recommended_items, relevant_items):\n",
    "    is_relevant = np.in1d(recommended_items, \n",
    "                          relevant_items, \n",
    "                          assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    ap_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return ap_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function **evaluate** which computes the RMSE and the ranking metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rec_object, train, test, \n",
    "             at = 20, thr_relevant= 4):\n",
    "    \"\"\" Perfomance evaluation \"\"\"\n",
    "    \n",
    "    # RMSE evaluation\n",
    "    ids_to_estimate = zip(test.user_id, test.movie_id)\n",
    "    y_estimated = np.array([rec_object.predict_score(u,i) \n",
    "                          if u in train.user_id else 3 \n",
    "                          for (u,i) in ids_to_estimate ])\n",
    "    \n",
    "    y_real = test.rating.values\n",
    "    rmse = compute_rmse(y_estimated, y_real)\n",
    "    \n",
    "    print(\"RMSE: {:.4f}\".format(rmse))\n",
    "          \n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_AP = 0.0\n",
    "\n",
    "    num_eval = 0\n",
    "\n",
    "    for user_id in tqdm(test.user_id.unique()):\n",
    "\n",
    "        relevant_items = test[(test.user_id==user_id )&( test.rating>=thr_relevant)].movie_id.values\n",
    "\n",
    "        if len(relevant_items)>0:\n",
    "\n",
    "            recommended_items = rec_object.predict_top(user_id, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_precision += precision(recommended_items, relevant_items)\n",
    "            cumulative_recall += recall(recommended_items, relevant_items)\n",
    "            cumulative_AP += AP(recommended_items, relevant_items)\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    MAP = cumulative_AP / num_eval\n",
    "\n",
    "    print(\"Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, MAP)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System \n",
    "\n",
    "### Naïve Recommender System \n",
    "\n",
    "First of all, let's build our naive recommender: A random recommender system.\n",
    "\n",
    "We can define our recommender system with a Python class. This class consists of a constructor and three methods: `fit`, `predict_score` and `predict_top`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRecommender():\n",
    "\n",
    "    def fit(self, train):\n",
    "        self.items = train.title.unique()\n",
    "    \n",
    "    def predict_score(self, user_id, movie_id):\n",
    "        '''Given a user_id and movie_id predict its score'''\n",
    "        return np.random.uniform(1,5)\n",
    "    \n",
    "    def predict_top(self, user_id, at=5):\n",
    "        '''Given a user_id predicts its top 'at' movies'''\n",
    "        recommended_items = np.random.choice(self.items, at)\n",
    "\n",
    "        return recommended_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create the object and train it using training data. We can do it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model = RandomRecommender()\n",
    "random_model.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can evaluate it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.6233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 943/943 [00:00<00:00, 1718.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.0000, Recall = 0.0000, MAP = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(random_model,X_train,X_test, at = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random model has achieved an RMSE of 1.6241 and Precision, Recall and MAP of 0.0\\%. These are the numbers we need to beat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Recommender System \n",
    "\n",
    "Similarly, we can build our collaborative recommender system with a Python class. This class consists of a constructor and three methods: `fit`, `predict_score` and `predict_top`. In this case,  the `fit` will be in charge to compute and store the users similarities into a Python dictionary. This is a really simple method but quite expensive in terms of computation when dealing with a large dataset. We decided to show one of the most basic schemes in order to implement it. More complex algorithms can be used in order to improve the computations cost. Moreover, online strategies can be used when dealing with a really dynamic problems. In the \\texttt{predict} the score for a movie and a user is estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering:\n",
    "    \"\"\" Collaborative filtering model \"\"\"\n",
    "    \n",
    "    def __init__(self, similarity = 'pearson'):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.sim_method=similarity\n",
    "        self.sim = {}\n",
    "\n",
    "    def fit(self, train):\n",
    "        \"\"\" Prepare data structures for estimation. Similarity matrix for users \"\"\"\n",
    "        print(\"Learning...\")\n",
    "        self.train = train\n",
    "        allUsers = set(self.train['user_id'])\n",
    "        \n",
    "        for usr_1 in tqdm(allUsers):\n",
    "            self.sim.setdefault(usr_1, {})\n",
    "            a = self.train[self.train.user_id == usr_1][['movie_id']]\n",
    "            data_reduced = pd.merge(self.train, a , on='movie_id') \n",
    "            for usr_2 in allUsers:\n",
    "                # Avoid our-self\n",
    "                if usr_1 == usr_2: continue\n",
    "                self.sim.setdefault(usr_2, {})\n",
    "                if(usr_1 in self.sim[usr_2]):\n",
    "                    continue # since is a simetric matrix\n",
    "                sim = user_sim(data_reduced, usr_1, usr_2, \n",
    "                               method = self.sim_method)\n",
    "                if(sim < 0):\n",
    "                    self.sim[usr_1][usr_2]=0\n",
    "                    self.sim[usr_2][usr_1]=0\n",
    "                else:\n",
    "                    self.sim[usr_1][usr_2]=sim\n",
    "                    self.sim[usr_2][usr_1]=sim\n",
    "                \n",
    "    def predict_score(self, usr_id, movie_id):\n",
    "        ''' Given a user_id and movie_id it predicts its score'''\n",
    "        seen = self.train[self.train.movie_id == movie_id]\n",
    "        rating_num, rating_den = 0.0, 0.0\n",
    "        allUsers = set(seen['user_id'])\n",
    "        for other in allUsers:\n",
    "            if usr_id == other: continue \n",
    "            rating_num += self.sim[usr_id][other] * float(seen[seen['user_id']==other]['rating'])\n",
    "            rating_den += self.sim[usr_id][other]\n",
    "        if rating_den == 0: \n",
    "            if self.train.rating[self.train['movie_id']==movie_id].mean()>0:\n",
    "                # return the mean movie rating if there is no similar for the computation\n",
    "                return self.train.rating[self.train['movie_id']==movie_id].mean()\n",
    "            else:# return mean user rating \n",
    "                return self.train.rating[self.train['user_id']==usr_id].mean()\n",
    "        return rating_num/rating_den\n",
    "\n",
    "    def predict_top(self, usr_id, at=5, remove_seen=True):\n",
    "        '''Given a usr_id predict its top 'at' movies'''\n",
    "        seen_items = self.train[self.train.user_id==usr_id].movie_id.values\n",
    "        unseen_items = set(self.train.movie_id.values) - set(seen_items)\n",
    "\n",
    "        predictions = [(item_id,self.predict_score(usr_id,item_id)) for item_id in unseen_items]\n",
    "\n",
    "        sorted_predictions = sorted(predictions, key=lambda x: x[1],reverse = True)[:at]\n",
    "        return [i[0] for i in sorted_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering:\n",
    "    \"\"\" Collaborative filtering model \"\"\"\n",
    "    \n",
    "    def __init__(self, similarity = 'pearson'):\n",
    "        '''Constructor'''\n",
    "        self.sim_method=similarity\n",
    "        self.sim = {}\n",
    "\n",
    "    def fit(self, train):\n",
    "        '''Prepare data structures for estimation'''\n",
    "        self.train = train\n",
    "        allUsers = set(self.train['user_id'])\n",
    "        \n",
    "        # Create a dictionary with user, movie, rating\n",
    "        self.seen_movies = {user: {} for user in allUsers}\n",
    "        \n",
    "        for user in allUsers:\n",
    "            user_ratings = train[train.user_id == user][['movie_id', 'rating']]\n",
    "            self.seen_movies[user] = dict(zip(user_ratings['movie_id'], user_ratings['rating']))\n",
    "    \n",
    "        for usr_1 in tqdm(allUsers):\n",
    "            self.sim.setdefault(usr_1, {})\n",
    "            a = self.train[self.train.user_id == usr_1][['movie_id']]\n",
    "            data_reduced = pd.merge(self.train, a , on='movie_id') \n",
    "            \n",
    "            for usr_2 in allUsers:\n",
    "                # Avoid comparing a user with themselves\n",
    "                if usr_1 == usr_2: \n",
    "                    continue      \n",
    "                self.sim.setdefault(usr_2, {})\n",
    "                if(usr_1 in self.sim[usr_2]):\n",
    "                    continue # since is a simetric matrix\n",
    "                sim = user_sim(data_reduced, usr_1, usr_2, \n",
    "                               method = self.sim_method)\n",
    "                if(sim < 0):\n",
    "                    self.sim[usr_1][usr_2]=0\n",
    "                    self.sim[usr_2][usr_1]=0\n",
    "                else:\n",
    "                    self.sim[usr_1][usr_2]=sim\n",
    "                    self.sim[usr_2][usr_1]=sim\n",
    "        \n",
    "        \n",
    "    def predict_score(self, usr_id, movie_id):\n",
    "        ''' Given a user_id and movie_id it predicts its score'''\n",
    "        seen = self.train[self.train.movie_id == movie_id]\n",
    "        rating_num, rating_den = 0.0, 0.0\n",
    "        allUsers = set(seen['user_id'])\n",
    "        # Iterate through all users who have seen the movie\n",
    "        for other in allUsers:\n",
    "            if usr_id == other:\n",
    "                continue  # Skip the current user, as self-similarity is not needed\n",
    "            \n",
    "            similarity = self.sim[usr_id][other]\n",
    "            rating = float(self.seen_movies[other][movie_id])\n",
    "            \n",
    "            rating_num += similarity * rating\n",
    "            rating_den += similarity\n",
    "            \n",
    "        # If the denominator is zero (no similar users), handle the case\n",
    "        if rating_den == 0: \n",
    "            if self.train.rating[self.train['movie_id'] == movie_id].mean()>0:\n",
    "                # return the mean movie rating if there is no similar for the computation\n",
    "                return self.train.rating[self.train['movie_id'] == movie_id].mean()\n",
    "            else:# return mean user rating \n",
    "                return self.train.rating[self.train['user_id'] == usr_id].mean()\n",
    "       \n",
    "        # return the predicted score\n",
    "        return rating_num/rating_den\n",
    "\n",
    "    def predict_top(self, usr_id, at=20, remove_seen=True):\n",
    "        '''Given a usr_id predict its top 'at' movies'''\n",
    "        # Get the movies already seen by the user\n",
    "        seen_items = set(self.train[self.train.user_id == usr_id].movie_id.values)\n",
    "\n",
    "        # Get the set of unseen items\n",
    "        unseen_items = set(self.train.movie_id.values) - seen_items\n",
    "\n",
    "        # Generate predictions for unseen items\n",
    "        predictions = [(item_id, self.predict_score(usr_id, item_id)) for item_id in unseen_items]\n",
    "\n",
    "        # Sort predictions by score in descending order and select the top 'at' items\n",
    "        sorted_predictions = sorted(predictions, key=lambda x: x[1], reverse=True)[:at]\n",
    "\n",
    "        # Extract the item IDs from the sorted predictions\n",
    "        top_items = [item_id for item_id, _ in sorted_predictions]\n",
    "        \n",
    "        return top_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 943/943 [14:02<00:00,  1.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.7884295266215537"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_model = CollaborativeFiltering()\n",
    "cf_model.fit(X_train)\n",
    "cf_model.predict_score(usr_id = 2, movie_id = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 943/943 [07:32<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.0231, Recall = 0.0829, MAP = 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# It can take a lot of time..\n",
    "evaluate(cf_model,X_train,X_test, at = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-success\">**EXERCISE 1**<p>\n",
    "Modify the Recomender System using as a prediction function the following equation:\n",
    "$$pred(a,p) = \\bar{r_a} + \\frac{\\sum_{b \\in N}{sim(a,b)*(r_{b,p}-\\bar{r_b})}}{\\sum_{b \\in N}{sim(a,b)}}$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering_Ex1(CollaborativeFiltering):\n",
    "    def predict_score(self, usr_id, movie_id, N = 10):\n",
    "        ''' Given a user_id and movie_id it predicts its score'''\n",
    "        seen = self.train[self.train['movie_id'] ==movie_id]\n",
    "        rating_num, rating_den = 0.0, 0.0\n",
    "        allUsers = set(seen['user_id'])\n",
    "        \n",
    "        for other in allUsers:\n",
    "            if usr_id == other:\n",
    "                continue  # Skip the current user, as self-similarity is not needed\n",
    "            similarity = self.sim[usr_id][other]\n",
    "            rating = float(self.seen_movies[other][movie_id])\n",
    "            mean_user_rating = np.mean([self.seen_movies[other][key] \n",
    "                                        for key in self.seen_movies[other]])\n",
    "            rating_num += similarity * float(rating - mean_user_rating)  \n",
    "            rating_den += similarity\n",
    "        if rating_den == 0: \n",
    "            if self.train.rating[self.train['movie_id'] == movie_id].mean()>0:\n",
    "                # return the mean movie rating if there is no similar for the computation\n",
    "                return self.train.rating[self.train['movie_id'] == movie_id].mean()\n",
    "            else:\n",
    "                # else return mean user rating \n",
    "                return self.train.rating[self.train['user_id'] == usr_id].mean()\n",
    "        mean_rating_user = self.train[self.train.user_id==usr_id].rating.mean()\n",
    "        return mean_rating_user + rating_num/rating_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 943/943 [14:18<00:00,  1.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.8641131577496144"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_model_v2 = CollaborativeFiltering_Ex1()\n",
    "cf_model_v2.fit(X_train)\n",
    "cf_model_v2.predict_score(usr_id = 2, movie_id = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 943/943 [41:33<00:00,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.0112, Recall = 0.0406, MAP = 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(cf_model_v2, X_train, X_test, at = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**EXERCISE 2:**<br>\n",
    "Modify the recomender system from the previous exercice, with one that in order to estimate the score of a movie B for the user A only uses the subset of the N most similar users to user A. Define N as a parameter of the Recoomender.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering_Ex2(CollaborativeFiltering):\n",
    "    def predict_score(self, usr_id, movie_id, N = 10):\n",
    "        seen = self.train[self.train['movie_id'] ==movie_id]\n",
    "        rating_num, rating_den = 0.0, 0.0\n",
    "        allUsers = set(seen['user_id'])\n",
    "        \n",
    "        # SELECT TOP USERS\n",
    "        top_users = sorted([(self.sim[usr_id][other],other) \n",
    "                            for other in seen.user_id.values \n",
    "                            if other in self.sim[usr_id]])[::-1]\n",
    "        \n",
    "        for sim, other in top_users[:N]:\n",
    "            if usr_id == other: continue \n",
    "                \n",
    "            similarity = self.sim[usr_id][other]\n",
    "            rating = float(self.seen_movies[other][movie_id])\n",
    "            mean_user_rating = np.mean([self.seen_movies[other][key] \n",
    "                                        for key in self.seen_movies[other]])\n",
    "            \n",
    "            rating_num += similarity * float(rating - mean_user_rating)  \n",
    "            rating_den += similarity\n",
    "        if rating_den == 0: \n",
    "            if self.train.rating[self.train['movie_id']==movie_id].mean()>0:\n",
    "                # return the mean movie rating if there is no similar for the computation\n",
    "                return self.train.rating[self.train['movie_id']==movie_id].mean()\n",
    "            else:\n",
    "                # else return mean user rating \n",
    "                return self.train.rating[self.train['user_id'] == usr_id].mean()\n",
    "        mean_rating_user = self.train[self.train.user_id == usr_id].rating.mean()\n",
    "        return mean_rating_user + rating_num/rating_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 943/943 [14:18<00:00,  1.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.214813747061439"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_model_v3 = CollaborativeFiltering_Ex2()\n",
    "cf_model_v3.fit(X_train)\n",
    "cf_model_v3.predict_score(usr_id = 2, movie_id = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 943/943 [22:27<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.0130, Recall = 0.0455, MAP = 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(cf_model_v3, X_train, X_test, at = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-success\">**EXERCISE 4**<p>\n",
    "Modify the similarity function with the following:\n",
    "$$new\\_sim(a,b) = sim(a,b) * \\frac{min(50,|P_{ab}|)}{50} $$\n",
    "where $|P_{ab}|$ is the number of common items with user $a$ and user $b$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_sim(df, user_1, user_2, \n",
    "             min_common_items = 1, \n",
    "             method = 'pearson',\n",
    "             min_common = 20):\n",
    "    # GET MOVIES OF USER1\n",
    "    mov_usr1 = df[df['user_id'] == user_1 ]\n",
    "    # GET MOVIES OF USER2\n",
    "    mov_usr2 = df[df['user_id'] == user_2 ]\n",
    "    \n",
    "    # FIND SHARED FILMS\n",
    "    df_shared = pd.merge(mov_usr1, mov_usr2, on='movie_id')  \n",
    "    n_common = df_shared.shape[0]\n",
    "    \n",
    "    # If there is no enough common items to compute similarity\n",
    "    if(n_common<=min_common_items):\n",
    "        return 0\n",
    "    \n",
    "    penalty = min(min_common,n_common)/min_common\n",
    "    if method =='pearson':\n",
    "        res=pearsonr(df_shared['rating_x'],df_shared['rating_y'])[0]\n",
    "        if(np.isnan(res)):\n",
    "            return 0\n",
    "        return res * penalty\n",
    "    \n",
    "    elif method =='euclidean':\n",
    "        res = 1.0/(1.0+euclidean(df_shared['rating_x'],\n",
    "                                  df_shared['rating_y'])) \n",
    "        return res * penalty\n",
    "    else:\n",
    "        print(\"method not defined\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 943/943 [14:07<00:00,  1.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.9092867613921407"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_model_v4 = CollaborativeFiltering_Ex1()\n",
    "cf_model_v4.fit(X_train)\n",
    "cf_model_v4.predict_score(usr_id = 2, movie_id = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▍                                  | 148/943 [05:57<31:59,  2.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate(cf_model_v4,X_train,X_test)\n",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(rec_object, train, test, at, thr_relevant)\u001b[0m\n\u001b[1;32m     24\u001b[0m relevant_items \u001b[38;5;241m=\u001b[39m test[(test\u001b[38;5;241m.\u001b[39muser_id\u001b[38;5;241m==\u001b[39muser_id )\u001b[38;5;241m&\u001b[39m( test\u001b[38;5;241m.\u001b[39mrating\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mthr_relevant)]\u001b[38;5;241m.\u001b[39mmovie_id\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(relevant_items)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     recommended_items \u001b[38;5;241m=\u001b[39m rec_object\u001b[38;5;241m.\u001b[39mpredict_top(user_id, at\u001b[38;5;241m=\u001b[39mat)\n\u001b[1;32m     29\u001b[0m     num_eval\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     31\u001b[0m     cumulative_precision \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m precision(recommended_items, relevant_items)\n",
      "Cell \u001b[0;32mIn[20], line 79\u001b[0m, in \u001b[0;36mCollaborativeFiltering.predict_top\u001b[0;34m(self, usr_id, at, remove_seen)\u001b[0m\n\u001b[1;32m     76\u001b[0m unseen_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mmovie_id\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;241m-\u001b[39m seen_items\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Generate predictions for unseen items\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [(item_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_score(usr_id, item_id)) \u001b[38;5;28;01mfor\u001b[39;00m item_id \u001b[38;5;129;01min\u001b[39;00m unseen_items]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Sort predictions by score in descending order and select the top 'at' items\u001b[39;00m\n\u001b[1;32m     82\u001b[0m sorted_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(predictions, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:at]\n",
      "Cell \u001b[0;32mIn[20], line 79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m unseen_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mmovie_id\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;241m-\u001b[39m seen_items\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Generate predictions for unseen items\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [(item_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_score(usr_id, item_id)) \u001b[38;5;28;01mfor\u001b[39;00m item_id \u001b[38;5;129;01min\u001b[39;00m unseen_items]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Sort predictions by score in descending order and select the top 'at' items\u001b[39;00m\n\u001b[1;32m     82\u001b[0m sorted_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(predictions, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:at]\n",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m, in \u001b[0;36mCollaborativeFiltering_Ex1.predict_score\u001b[0;34m(self, usr_id, movie_id, N)\u001b[0m\n\u001b[1;32m     14\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim[usr_id][other]\n\u001b[1;32m     15\u001b[0m rating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen_movies[other][movie_id])\n\u001b[0;32m---> 16\u001b[0m mean_user_rating \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen_movies[other][key] \n\u001b[1;32m     17\u001b[0m                             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen_movies[other]])\n\u001b[1;32m     18\u001b[0m rating_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m similarity \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mfloat\u001b[39m(rating \u001b[38;5;241m-\u001b[39m mean_user_rating)  \n\u001b[1;32m     19\u001b[0m rating_den \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m similarity\n",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim[usr_id][other]\n\u001b[1;32m     15\u001b[0m rating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen_movies[other][movie_id])\n\u001b[0;32m---> 16\u001b[0m mean_user_rating \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen_movies[other][key] \n\u001b[1;32m     17\u001b[0m                             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen_movies[other]])\n\u001b[1;32m     18\u001b[0m rating_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m similarity \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mfloat\u001b[39m(rating \u001b[38;5;241m-\u001b[39m mean_user_rating)  \n\u001b[1;32m     19\u001b[0m rating_den \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m similarity\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(cf_model_v4,X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
